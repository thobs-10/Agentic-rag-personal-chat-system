# Model configuration for embeddings
model:
  name: all-MiniLM-L6-v2
  device: cpu
  batch_size: 32
  vector_size: 384

# Database configuration
database:
  url: http://localhost:6333
  collections:
    - name: technical_collection
      data_dir: data/technical
    - name: personal_collection
      data_dir: data/personal
  recreate_collection: false
  distance: Cosine

# LLM configuration
llm:
  model_name: "Qwen/Qwen3-Coder-30B-A3B-Instruct"
  temperature: 0.1
  use_local: true
  device: "auto" # Will use CUDA if available, otherwise CPU
  max_tokens: 2048

# Pipeline configuration
pipeline:
  loading_strategy: langchain

# Text processing options
text:
  chunk_size: 500
  chunk_overlap: 50
  separator: "\n"

# Logging configuration
logging:
  level: INFO
  file: logs/ingestion.log
  format: "%(asctime)s | %(levelname)s | %(message)s"

# Environment settings
environment:
  zenml_server: http://127.0.0.1:8080
  temp_dir: /tmp/ingestion
